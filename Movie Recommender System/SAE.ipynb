{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AE.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6t5nPVdSpcD"
      },
      "source": [
        "# AutoEncoders\n",
        "\n",
        "# Importing the libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PY2EzeTwS60g"
      },
      "source": [
        "# Importing the dataset\n",
        "movies = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Boltzman/ml-1m/movies.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')\n",
        "users = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Boltzman/ml-1m/users.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')\n",
        "ratings = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Boltzman/ml-1m/ratings.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')\n",
        "\n",
        "# Preparing the training set and the test set\n",
        "training_set = pd.read_csv('/content/drive/My Drive/Colab Notebooks/AutoEncoders/ml-100k/ml-100k/u1.base', delimiter = '\\t')\n",
        "test_set = pd.read_csv('/content/drive/My Drive/Colab Notebooks/AutoEncoders/ml-100k/ml-100k/u1.test', delimiter = '\\t')\n",
        "training_set = np.array(training_set, dtype = 'int')\n",
        "test_set = np.array(test_set, dtype = 'int')\n",
        "\n",
        "# Getting the number of users and movies\n",
        "nb_users = int(max(max(training_set[:,0]), max(test_set[:,0])))\n",
        "nb_movies = int(max(max(training_set[:,1]), max(test_set[:,1])))\n",
        "\n",
        "# Converting the data into an array with users in lines and movies in columns\n",
        "def convert(data):\n",
        "    new_data = []\n",
        "    for id_users in range(1, nb_users + 1):\n",
        "        id_movies = data[:,1][data[:,0] == id_users]\n",
        "        id_ratings = data[:,2][data[:,0] == id_users]\n",
        "        ratings = np.zeros(nb_movies)\n",
        "        ratings[id_movies - 1] = id_ratings\n",
        "        new_data.append(list(ratings))\n",
        "    return new_data\n",
        "training_set = convert(training_set)\n",
        "test_set = convert(test_set)\n",
        "\n",
        "# Converting the data into Torch tensors\n",
        "training_set = torch.FloatTensor(training_set)\n",
        "test_set = torch.FloatTensor(test_set)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWBtAkjNW3VT"
      },
      "source": [
        "# Creating the architecture of the Neural Network\n",
        "class SAE(nn.Module):\n",
        "    def __init__(self, ):\n",
        "        super(SAE, self).__init__()\n",
        "        self.fc1 = nn.Linear(nb_movies, 20)\n",
        "        self.fc2 = nn.Linear(20, 10)\n",
        "        self.fc3 = nn.Linear(10, 20)\n",
        "        self.fc4 = nn.Linear(20, nb_movies)\n",
        "        self.activation = nn.Sigmoid()\n",
        "    def forward(self, x):\n",
        "        x = self.activation(self.fc1(x))\n",
        "        x = self.activation(self.fc2(x))\n",
        "        x = self.activation(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "sae = SAE()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.RMSprop(sae.parameters(), lr = 0.01, weight_decay = 0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pTTXswEzl-9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa55616d-3768-402d-9df1-8674fad0a7e4"
      },
      "source": [
        "# Training the SAE\n",
        "nb_epoch = 200\n",
        "for epoch in range (1, nb_epoch+1):\n",
        "    train_loss = 0\n",
        "    s = 0.\n",
        "    for id_user in range(nb_users):\n",
        "        input = Variable(training_set[id_user]).unsqueeze(0)\n",
        "        target = input.clone()\n",
        "        if torch.sum(target.data > 0) > 0 :\n",
        "            output = sae(input)\n",
        "            target.require_grad = False\n",
        "            output[target == 0] = 0\n",
        "            loss = criterion(output, target)\n",
        "            mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)\n",
        "            loss.backward()\n",
        "            train_loss += np.sqrt(loss.data.item()*mean_corrector)\n",
        "            s += 1.\n",
        "            optimizer.step()\n",
        "    print(\"epoch: \"+ str(epoch) + \" loss: \"+str(train_loss/s))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 1 loss: 1.7720670899728646\n",
            "epoch: 2 loss: 1.096816029499128\n",
            "epoch: 3 loss: 1.0533041812493262\n",
            "epoch: 4 loss: 1.0383477828221173\n",
            "epoch: 5 loss: 1.0308013534164875\n",
            "epoch: 6 loss: 1.0265501352810142\n",
            "epoch: 7 loss: 1.0236692375286607\n",
            "epoch: 8 loss: 1.0218415573322486\n",
            "epoch: 9 loss: 1.0208105255667557\n",
            "epoch: 10 loss: 1.019407587359843\n",
            "epoch: 11 loss: 1.0187731783478535\n",
            "epoch: 12 loss: 1.0183718958770325\n",
            "epoch: 13 loss: 1.0178283026263752\n",
            "epoch: 14 loss: 1.017302212885839\n",
            "epoch: 15 loss: 1.0171401558499702\n",
            "epoch: 16 loss: 1.016689754245841\n",
            "epoch: 17 loss: 1.0167443227458126\n",
            "epoch: 18 loss: 1.0165165439840151\n",
            "epoch: 19 loss: 1.0163418684544616\n",
            "epoch: 20 loss: 1.0159326713816812\n",
            "epoch: 21 loss: 1.01609063770526\n",
            "epoch: 22 loss: 1.0160295767359768\n",
            "epoch: 23 loss: 1.0158813152917174\n",
            "epoch: 24 loss: 1.0157681781096197\n",
            "epoch: 25 loss: 1.0157543645500264\n",
            "epoch: 26 loss: 1.015608926654231\n",
            "epoch: 27 loss: 1.015312471004435\n",
            "epoch: 28 loss: 1.0152465794869103\n",
            "epoch: 29 loss: 1.0137375591562772\n",
            "epoch: 30 loss: 1.011722143263167\n",
            "epoch: 31 loss: 1.0093491274125026\n",
            "epoch: 32 loss: 1.0097991271455062\n",
            "epoch: 33 loss: 1.0051082533549565\n",
            "epoch: 34 loss: 1.004965378157189\n",
            "epoch: 35 loss: 1.001921354579528\n",
            "epoch: 36 loss: 1.0000629884815093\n",
            "epoch: 37 loss: 0.9975588669189834\n",
            "epoch: 38 loss: 0.9968195495584535\n",
            "epoch: 39 loss: 0.994303633502235\n",
            "epoch: 40 loss: 0.9905171488384404\n",
            "epoch: 41 loss: 0.9893767782171387\n",
            "epoch: 42 loss: 0.9904725631916765\n",
            "epoch: 43 loss: 0.9878509782167196\n",
            "epoch: 44 loss: 0.9878155322190315\n",
            "epoch: 45 loss: 0.9848391735288565\n",
            "epoch: 46 loss: 0.9821901150587774\n",
            "epoch: 47 loss: 0.9832478168922925\n",
            "epoch: 48 loss: 0.9818657421073291\n",
            "epoch: 49 loss: 0.9794682554377186\n",
            "epoch: 50 loss: 0.9753420330734489\n",
            "epoch: 51 loss: 0.9724022901548268\n",
            "epoch: 52 loss: 0.9769849322475971\n",
            "epoch: 53 loss: 0.9716435735935544\n",
            "epoch: 54 loss: 0.9689917873633359\n",
            "epoch: 55 loss: 0.9650325128720928\n",
            "epoch: 56 loss: 0.9666646646337116\n",
            "epoch: 57 loss: 0.9618168148030233\n",
            "epoch: 58 loss: 0.9628424908936868\n",
            "epoch: 59 loss: 0.9585696473280941\n",
            "epoch: 60 loss: 0.9596242899278815\n",
            "epoch: 61 loss: 0.9558050003209267\n",
            "epoch: 62 loss: 0.9568505855811171\n",
            "epoch: 63 loss: 0.9540728192739161\n",
            "epoch: 64 loss: 0.9543741380411488\n",
            "epoch: 65 loss: 0.9508401150226866\n",
            "epoch: 66 loss: 0.9516474923986874\n",
            "epoch: 67 loss: 0.948611736421825\n",
            "epoch: 68 loss: 0.9493708730962792\n",
            "epoch: 69 loss: 0.947109666925885\n",
            "epoch: 70 loss: 0.9479656454534333\n",
            "epoch: 71 loss: 0.9446023212099686\n",
            "epoch: 72 loss: 0.9459927326417258\n",
            "epoch: 73 loss: 0.9436846995206506\n",
            "epoch: 74 loss: 0.944114725332709\n",
            "epoch: 75 loss: 0.9427799148780824\n",
            "epoch: 76 loss: 0.9435842810100793\n",
            "epoch: 77 loss: 0.9410780100215518\n",
            "epoch: 78 loss: 0.9424501100160172\n",
            "epoch: 79 loss: 0.9397179411254794\n",
            "epoch: 80 loss: 0.941109805934873\n",
            "epoch: 81 loss: 0.9385465485013845\n",
            "epoch: 82 loss: 0.9403998610958493\n",
            "epoch: 83 loss: 0.9376865794210176\n",
            "epoch: 84 loss: 0.9391544333371195\n",
            "epoch: 85 loss: 0.9365277439004487\n",
            "epoch: 86 loss: 0.9381979775646162\n",
            "epoch: 87 loss: 0.9357872537623083\n",
            "epoch: 88 loss: 0.9377136748186115\n",
            "epoch: 89 loss: 0.9352443114756233\n",
            "epoch: 90 loss: 0.936732904584082\n",
            "epoch: 91 loss: 0.9341809962193821\n",
            "epoch: 92 loss: 0.9362320476072333\n",
            "epoch: 93 loss: 0.9334775226971866\n",
            "epoch: 94 loss: 0.9352021588423148\n",
            "epoch: 95 loss: 0.9326601216464986\n",
            "epoch: 96 loss: 0.9345802573190546\n",
            "epoch: 97 loss: 0.9318280393229026\n",
            "epoch: 98 loss: 0.9335345542077642\n",
            "epoch: 99 loss: 0.9311348476072879\n",
            "epoch: 100 loss: 0.9327331756590029\n",
            "epoch: 101 loss: 0.9307812897215104\n",
            "epoch: 102 loss: 0.9320304051753379\n",
            "epoch: 103 loss: 0.9301196600714303\n",
            "epoch: 104 loss: 0.9313320858641869\n",
            "epoch: 105 loss: 0.92909886768326\n",
            "epoch: 106 loss: 0.9304657022882022\n",
            "epoch: 107 loss: 0.9287627332701399\n",
            "epoch: 108 loss: 0.9299118676623088\n",
            "epoch: 109 loss: 0.9281893899544782\n",
            "epoch: 110 loss: 0.9293305311642482\n",
            "epoch: 111 loss: 0.9277310554134947\n",
            "epoch: 112 loss: 0.9288711635741682\n",
            "epoch: 113 loss: 0.9268914087321419\n",
            "epoch: 114 loss: 0.9280254060067072\n",
            "epoch: 115 loss: 0.9263682193215439\n",
            "epoch: 116 loss: 0.9273886325194388\n",
            "epoch: 117 loss: 0.9255960942658789\n",
            "epoch: 118 loss: 0.9267666179133905\n",
            "epoch: 119 loss: 0.9254737951503651\n",
            "epoch: 120 loss: 0.9263408990814106\n",
            "epoch: 121 loss: 0.9247461375548189\n",
            "epoch: 122 loss: 0.9258546070524232\n",
            "epoch: 123 loss: 0.9241775540564752\n",
            "epoch: 124 loss: 0.9248832267350913\n",
            "epoch: 125 loss: 0.9237385548540429\n",
            "epoch: 126 loss: 0.924664317418246\n",
            "epoch: 127 loss: 0.9230128108458676\n",
            "epoch: 128 loss: 0.9240422840494126\n",
            "epoch: 129 loss: 0.9224660570703139\n",
            "epoch: 130 loss: 0.9236726037807597\n",
            "epoch: 131 loss: 0.9221485504782874\n",
            "epoch: 132 loss: 0.9228252600994987\n",
            "epoch: 133 loss: 0.9214523940076406\n",
            "epoch: 134 loss: 0.9226361191401897\n",
            "epoch: 135 loss: 0.9210095188003082\n",
            "epoch: 136 loss: 0.9220842022843642\n",
            "epoch: 137 loss: 0.920613710344244\n",
            "epoch: 138 loss: 0.9216421074793246\n",
            "epoch: 139 loss: 0.9201819374630383\n",
            "epoch: 140 loss: 0.9211030185918532\n",
            "epoch: 141 loss: 0.9197179613246793\n",
            "epoch: 142 loss: 0.920704187717135\n",
            "epoch: 143 loss: 0.9192041121711757\n",
            "epoch: 144 loss: 0.9202695279200147\n",
            "epoch: 145 loss: 0.9187253789196524\n",
            "epoch: 146 loss: 0.9197066549376454\n",
            "epoch: 147 loss: 0.9183372541763548\n",
            "epoch: 148 loss: 0.9193932309809815\n",
            "epoch: 149 loss: 0.9179676203791953\n",
            "epoch: 150 loss: 0.9189345785456838\n",
            "epoch: 151 loss: 0.917435058186491\n",
            "epoch: 152 loss: 0.9184665531424225\n",
            "epoch: 153 loss: 0.9170609302847111\n",
            "epoch: 154 loss: 0.9180911539090038\n",
            "epoch: 155 loss: 0.9168625749895303\n",
            "epoch: 156 loss: 0.9177261297544685\n",
            "epoch: 157 loss: 0.9161734940772779\n",
            "epoch: 158 loss: 0.9174093891893788\n",
            "epoch: 159 loss: 0.9159202187706479\n",
            "epoch: 160 loss: 0.9170327067885715\n",
            "epoch: 161 loss: 0.9155503547157082\n",
            "epoch: 162 loss: 0.9166693915564077\n",
            "epoch: 163 loss: 0.915336449089815\n",
            "epoch: 164 loss: 0.9163125159237726\n",
            "epoch: 165 loss: 0.9149514920615297\n",
            "epoch: 166 loss: 0.9159215987678226\n",
            "epoch: 167 loss: 0.914553294040952\n",
            "epoch: 168 loss: 0.9156322630536838\n",
            "epoch: 169 loss: 0.914157084324901\n",
            "epoch: 170 loss: 0.9152286379507579\n",
            "epoch: 171 loss: 0.9137040524112195\n",
            "epoch: 172 loss: 0.9148876943995496\n",
            "epoch: 173 loss: 0.9134881486176356\n",
            "epoch: 174 loss: 0.9147535592500308\n",
            "epoch: 175 loss: 0.9133848336630923\n",
            "epoch: 176 loss: 0.9143235528991904\n",
            "epoch: 177 loss: 0.9131413818070676\n",
            "epoch: 178 loss: 0.9138482352395357\n",
            "epoch: 179 loss: 0.9127495810170643\n",
            "epoch: 180 loss: 0.9138051057925656\n",
            "epoch: 181 loss: 0.9124851314413797\n",
            "epoch: 182 loss: 0.9133188517390471\n",
            "epoch: 183 loss: 0.9121013727196613\n",
            "epoch: 184 loss: 0.9129183867735418\n",
            "epoch: 185 loss: 0.911808865891256\n",
            "epoch: 186 loss: 0.9124399922629274\n",
            "epoch: 187 loss: 0.9116142841676464\n",
            "epoch: 188 loss: 0.9124049772877405\n",
            "epoch: 189 loss: 0.9111821446943302\n",
            "epoch: 190 loss: 0.9121102710817093\n",
            "epoch: 191 loss: 0.9108574645097074\n",
            "epoch: 192 loss: 0.9116198259991825\n",
            "epoch: 193 loss: 0.9105854211304828\n",
            "epoch: 194 loss: 0.911395413592546\n",
            "epoch: 195 loss: 0.9101050594761019\n",
            "epoch: 196 loss: 0.9107946623289864\n",
            "epoch: 197 loss: 0.909792459526169\n",
            "epoch: 198 loss: 0.910404740393753\n",
            "epoch: 199 loss: 0.9093080034792825\n",
            "epoch: 200 loss: 0.9100491924482338\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2mhnkK39veN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6273f048-c100-4fa8-ed06-57a6e80d5537"
      },
      "source": [
        "# Testing the SAE\n",
        "test_loss = 0\n",
        "s = 0.\n",
        "for id_user in range(nb_users):\n",
        "    input = Variable(training_set[id_user]).unsqueeze(0)\n",
        "    target = Variable(test_set[id_user]).unsqueeze(0)\n",
        "    if torch.sum(target.data > 0) > 0 :\n",
        "        output = sae(input)\n",
        "        target.require_grad = False\n",
        "        output[target == 0] = 0\n",
        "        loss = criterion(output, target)\n",
        "        mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)\n",
        "        test_loss += np.sqrt(loss.data.item()*mean_corrector)\n",
        "        s += 1.\n",
        "        optimizer.step()\n",
        "print(\"test_loss: \"+str(test_loss/s))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test_loss: 0.9508767464106335\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAeXTXvr_bas"
      },
      "source": [
        "#Making Predictions with visualisation in the form of a table\n",
        "user_id = 0\n",
        "movie_title = movies.iloc[:nb_movies, 1:2]\n",
        "user_rating = training_set.data.numpy()[user_id, :].reshape(-1,1)\n",
        "user_target = test_set.data.numpy()[user_id, :].reshape(-1,1)\n",
        " \n",
        "user_input = Variable(training_set[user_id]).unsqueeze(0)\n",
        "predicted = sae(user_input)\n",
        "predicted = predicted.data.numpy().reshape(-1,1)\n",
        " \n",
        "# Join all info in one dataset\n",
        "result_array = np.hstack([movie_title, user_target, predicted])\n",
        "result_array = result_array[result_array[:, 1] > 0]\n",
        "result_df = pd.DataFrame(data=result_array, columns=['Movie', 'Target Rating', 'Predicted'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "ugtpWjHS1_G4",
        "outputId": "3847a82d-5985-4a4c-abea-4b605a9e0810"
      },
      "source": [
        "result_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Movie</th>\n",
              "      <th>Target Rating</th>\n",
              "      <th>Predicted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>GoldenEye (1995)</td>\n",
              "      <td>3</td>\n",
              "      <td>3.92705</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Dracula: Dead and Loving It (1995)</td>\n",
              "      <td>5</td>\n",
              "      <td>4.59359</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Nixon (1995)</td>\n",
              "      <td>5</td>\n",
              "      <td>4.01931</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Sense and Sensibility (1995)</td>\n",
              "      <td>3</td>\n",
              "      <td>3.39048</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Money Train (1995)</td>\n",
              "      <td>4</td>\n",
              "      <td>3.55919</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>131</th>\n",
              "      <td>Legends of the Fall (1994)</td>\n",
              "      <td>2</td>\n",
              "      <td>2.88937</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>132</th>\n",
              "      <td>Major Payne (1994)</td>\n",
              "      <td>4</td>\n",
              "      <td>4.21844</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>133</th>\n",
              "      <td>Little Odessa (1994)</td>\n",
              "      <td>1</td>\n",
              "      <td>2.57925</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>134</th>\n",
              "      <td>My Crazy Life (Mi vida loca) (1993)</td>\n",
              "      <td>4</td>\n",
              "      <td>2.70674</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>135</th>\n",
              "      <td>Man of the House (1995)</td>\n",
              "      <td>3</td>\n",
              "      <td>4.34351</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>136 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                   Movie Target Rating Predicted\n",
              "0                       GoldenEye (1995)             3   3.92705\n",
              "1     Dracula: Dead and Loving It (1995)             5   4.59359\n",
              "2                           Nixon (1995)             5   4.01931\n",
              "3           Sense and Sensibility (1995)             3   3.39048\n",
              "4                     Money Train (1995)             4   3.55919\n",
              "..                                   ...           ...       ...\n",
              "131           Legends of the Fall (1994)             2   2.88937\n",
              "132                   Major Payne (1994)             4   4.21844\n",
              "133                 Little Odessa (1994)             1   2.57925\n",
              "134  My Crazy Life (Mi vida loca) (1993)             4   2.70674\n",
              "135              Man of the House (1995)             3   4.34351\n",
              "\n",
              "[136 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VghEfFXL5OCK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}